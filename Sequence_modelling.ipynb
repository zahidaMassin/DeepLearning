{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence modelling \n",
    "\n",
    "## Coding tutorials\n",
    " #### [1.  The IMDb dataset](#coding_tutorial_1)\n",
    " #### [2. Padding and masking sequence data](#coding_tutorial_2)\n",
    " #### [3. The Embedding layer](#coding_tutorial_3)\n",
    " #### [4. The Embedding Projector](#coding_tutorial_4)\n",
    " #### [5. Recurrent neural network layers](#coding_tutorial_5)\n",
    " #### [6. Stacked RNNs and the Bidirectional wrapper](#coding_tutorial_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## The IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imdb\n",
    "\n",
    "import tensorflow.keras.datasets.imdb as imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download and assign the data set using load_data()\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the type of the data\n",
    "type(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the shape of the data\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element input\n",
    "# Notice encoding\n",
    "\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first dataset element output\n",
    "\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset with different options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with defaults\n",
    "imdb.load_data(path='imdb.npz',index_from =3)\n",
    "\n",
    "# ~/.keras/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([1, 194, 2, 194, 2, 78, 228, 5, 6, 2, 2, 2, 134, 26, 4, 715, 8, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 2, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2, 2, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 2, 4, 118, 9, 4, 130, 2, 19, 4, 2, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 2, 2, 398, 4, 2, 26, 2, 5, 163, 11, 2, 2, 4, 2, 9, 194, 775, 7, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 5, 4, 228, 9, 43, 2, 2, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 2, 228, 2, 5, 2, 656, 245, 2, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 9, 6, 371, 78, 22, 625, 64, 2, 9, 8, 168, 145, 23, 4, 2, 15, 16, 4, 2, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 4, 86, 320, 35, 534, 19, 263, 2, 2, 4, 2, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 2, 43, 645, 662, 8, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 8, 106, 14, 2, 2, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2, 51, 9, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 2, 9, 6, 2, 446, 2, 45, 2, 84, 2, 2, 21, 4, 912, 84, 2, 325, 725, 134, 2, 2, 84, 5, 36, 28, 57, 2, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 2, 14, 9, 31, 7, 4, 2, 2, 2, 2, 2, 18, 6, 20, 207, 110, 563, 12, 8, 2, 2, 8, 97, 6, 20, 53, 2, 74, 4, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 6, 2, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 2, 11, 4, 2, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2, 234, 2, 2, 7, 496, 4, 139, 929, 2, 2, 2, 5, 2, 18, 4, 2, 2, 250, 11, 2, 2, 4, 2, 2, 747, 2, 372, 2, 2, 541, 2, 7, 4, 59, 2, 4, 2, 2]),\n",
       "         list([1, 2, 2, 69, 72, 2, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 8, 4, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 2, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 4, 719, 2, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2, 101, 405, 39, 14, 2, 4, 2, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 2, 2, 9, 24, 6, 78, 2, 17, 2, 2, 21, 27, 2, 2, 5, 2, 2, 92, 2, 4, 2, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 2, 4, 631, 2, 5, 2, 272, 191, 2, 6, 2, 8, 2, 2, 2, 544, 5, 383, 2, 848, 2, 2, 497, 2, 8, 2, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 2, 72, 7, 51, 6, 2, 22, 4, 204, 131, 9])],\n",
       "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 2, 6, 176, 7, 2, 88, 12, 2, 23, 2, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 2, 2, 4, 2, 2, 109, 2, 21, 4, 22, 2, 8, 6, 2, 2, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 2, 5, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 4, 405, 5, 2, 7, 27, 85, 108, 131, 4, 2, 2, 2, 405, 9, 2, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 2, 2, 45, 407, 31, 7, 41, 2, 105, 21, 59, 299, 12, 38, 950, 5, 2, 15, 45, 629, 488, 2, 127, 6, 52, 292, 17, 4, 2, 185, 132, 2, 2, 2, 488, 2, 47, 6, 392, 173, 4, 2, 2, 270, 2, 4, 2, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2, 7, 2, 861, 2, 5, 2, 30, 2, 2, 56, 4, 841, 5, 990, 692, 8, 4, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 9, 31, 7, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 9, 2, 21, 45, 2, 5, 45, 252, 8, 2, 6, 565, 921, 2, 39, 4, 529, 48, 25, 181, 8, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 2, 25, 203, 28, 8, 818, 12, 125, 4, 2]),\n",
       "         list([1, 111, 748, 2, 2, 2, 2, 4, 87, 2, 2, 7, 31, 318, 2, 7, 4, 498, 2, 748, 63, 29, 2, 220, 686, 2, 5, 17, 12, 575, 220, 2, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 2, 8, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 5, 2, 98, 31, 2, 33, 6, 58, 14, 2, 2, 8, 4, 365, 7, 2, 2, 356, 346, 4, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 6, 58, 54, 2, 431, 748, 7, 32, 2, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 2, 2, 2, 2, 8, 847, 8, 2, 121, 31, 7, 27, 86, 2, 2, 16, 6, 465, 993, 2, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 2, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 4, 2, 2, 74, 476, 37, 62, 91, 2, 169, 4, 2, 2, 146, 655, 2, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 2, 18, 631, 2, 797, 7, 4, 2, 71, 348, 425, 2, 2, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2, 2, 7, 4, 2, 10, 10, 263, 787, 9, 270, 11, 6, 2, 4, 2, 2, 121, 4, 2, 26, 2, 19, 68, 2, 5, 28, 446, 6, 318, 2, 8, 67, 51, 36, 70, 81, 8, 2, 2, 36, 2, 8, 2, 2, 18, 6, 711, 4, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 9, 2, 17, 6, 2, 428, 2, 232, 11, 4, 2, 37, 272, 40, 2, 247, 30, 656, 6, 2, 54, 2, 2, 98, 6, 2, 40, 558, 37, 2, 98, 4, 2, 2, 15, 14, 9, 57, 2, 5, 2, 6, 275, 711, 2, 2, 2, 98, 6, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 4, 2, 2, 90, 19, 6, 2, 7, 2, 2, 2, 4, 2, 2, 930, 8, 508, 90, 4, 2, 8, 4, 2, 17, 2, 2, 2, 4, 2, 8, 2, 189, 4, 2, 2, 2, 4, 2, 5, 95, 271, 23, 6, 2, 2, 2, 2, 33, 2, 6, 425, 2, 2, 2, 2, 7, 4, 2, 2, 469, 4, 2, 54, 4, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 5, 2, 68, 2, 19, 2, 2, 4, 2, 7, 263, 65, 2, 34, 6, 2, 2, 43, 159, 29, 9, 2, 9, 387, 73, 195, 584, 10, 10, 2, 4, 58, 810, 54, 14, 2, 117, 22, 16, 93, 5, 2, 4, 192, 15, 12, 16, 93, 34, 6, 2, 2, 33, 4, 2, 7, 15, 2, 2, 2, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 7, 819, 4, 22, 2, 17, 6, 2, 787, 7, 2, 2, 2, 100, 30, 4, 2, 2, 2, 2, 42, 2, 11, 4, 2, 42, 101, 704, 7, 101, 999, 15, 2, 94, 2, 180, 5, 9, 2, 34, 2, 45, 6, 2, 22, 60, 6, 2, 31, 11, 94, 2, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 2, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 4, 2, 910, 769, 2, 2, 395, 2, 5, 2, 11, 119, 2, 89, 2, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2, 284, 2, 2, 37, 315, 4, 226, 20, 272, 2, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2, 7, 743, 46, 2, 9, 2, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 2, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 6, 2, 7, 470]),\n",
       "         list([1, 6, 52, 2, 430, 22, 9, 220, 2, 8, 28, 2, 519, 2, 6, 769, 15, 47, 6, 2, 2, 8, 114, 5, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 5, 6, 364, 350, 4, 184, 2, 9, 133, 2, 11, 2, 2, 21, 4, 2, 2, 570, 50, 2, 2, 9, 6, 2, 17, 6, 2, 2, 21, 17, 6, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 2, 19, 4, 78, 173, 7, 27, 2, 2, 2, 718, 2, 9, 6, 2, 17, 210, 5, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 2, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 2, 382, 4, 91, 2, 439, 19, 14, 20, 9, 2, 2, 2, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0])))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the vocabulary to the top 500 words using num_words\n",
    "\n",
    "imdb.load_data(num_words = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([2, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 2, 173, 36, 256, 2, 25, 100, 43, 838, 112, 50, 670, 2, 2, 35, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 2, 17, 546, 38, 13, 447, 2, 192, 50, 16, 2, 147, 2, 19, 14, 22, 2, 2, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 2, 62, 386, 12, 2, 316, 2, 106, 2, 2, 2, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 2, 25, 124, 51, 36, 135, 48, 25, 2, 33, 2, 22, 12, 215, 28, 77, 52, 2, 14, 407, 16, 82, 2, 2, 2, 107, 117, 2, 15, 256, 2, 2, 2, 2, 2, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 2, 2, 2, 2, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2, 56, 26, 141, 2, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 2, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 2, 88, 12, 16, 283, 2, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]),\n",
       "         list([2, 194, 2, 194, 2, 78, 228, 2, 2, 2, 2, 2, 134, 26, 2, 715, 2, 118, 2, 14, 394, 20, 13, 119, 954, 189, 102, 2, 207, 110, 2, 21, 14, 69, 188, 2, 30, 23, 2, 2, 249, 126, 93, 2, 114, 2, 2, 2, 2, 647, 2, 116, 2, 35, 2, 2, 229, 2, 340, 2, 2, 118, 2, 2, 130, 2, 19, 2, 2, 2, 89, 29, 952, 46, 37, 2, 455, 2, 45, 43, 38, 2, 2, 398, 2, 2, 26, 2, 2, 163, 11, 2, 2, 2, 2, 2, 194, 775, 2, 2, 2, 349, 2, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 2, 98, 2, 2, 228, 2, 43, 2, 2, 15, 299, 120, 2, 120, 174, 11, 220, 175, 136, 50, 2, 2, 228, 2, 2, 2, 656, 245, 2, 2, 2, 2, 131, 152, 491, 18, 2, 32, 2, 2, 14, 2, 2, 371, 78, 22, 625, 64, 2, 2, 2, 168, 145, 23, 2, 2, 15, 16, 2, 2, 2, 28, 2, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([2, 14, 47, 2, 30, 31, 2, 2, 249, 108, 2, 2, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 2, 2, 311, 12, 16, 2, 33, 75, 43, 2, 296, 2, 86, 320, 35, 534, 19, 263, 2, 2, 2, 2, 33, 89, 78, 12, 66, 16, 2, 360, 2, 2, 58, 316, 334, 11, 2, 2, 43, 645, 662, 2, 257, 85, 2, 42, 2, 2, 83, 68, 2, 15, 36, 165, 2, 278, 36, 69, 2, 780, 2, 106, 14, 2, 2, 18, 2, 22, 12, 215, 28, 610, 40, 2, 87, 326, 23, 2, 21, 23, 22, 12, 272, 40, 57, 31, 11, 2, 22, 47, 2, 2, 51, 2, 170, 23, 595, 116, 595, 2, 13, 191, 79, 638, 89, 2, 14, 2, 2, 106, 607, 624, 35, 534, 2, 227, 2, 129, 113]),\n",
       "         ...,\n",
       "         list([2, 11, 2, 230, 245, 2, 2, 2, 2, 446, 2, 45, 2, 84, 2, 2, 21, 2, 912, 84, 2, 325, 725, 134, 2, 2, 84, 2, 36, 28, 57, 2, 21, 2, 140, 2, 703, 2, 2, 84, 56, 18, 2, 14, 2, 31, 2, 2, 2, 2, 2, 2, 2, 18, 2, 20, 207, 110, 563, 12, 2, 2, 2, 2, 97, 2, 20, 53, 2, 74, 2, 460, 364, 2, 29, 270, 11, 960, 108, 45, 40, 29, 2, 395, 11, 2, 2, 500, 2, 2, 89, 364, 70, 29, 140, 2, 64, 2, 11, 2, 2, 26, 178, 2, 529, 443, 2, 2, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 2, 2, 10, 10, 288, 2, 2, 34, 2, 2, 2, 65, 496, 2, 231, 2, 790, 2, 2, 320, 234, 2, 234, 2, 2, 2, 496, 2, 139, 929, 2, 2, 2, 2, 2, 18, 2, 2, 2, 250, 11, 2, 2, 2, 2, 2, 747, 2, 372, 2, 2, 541, 2, 2, 2, 59, 2, 2, 2, 2]),\n",
       "         list([2, 2, 2, 69, 72, 2, 13, 610, 930, 2, 12, 582, 23, 2, 16, 484, 685, 54, 349, 11, 2, 2, 45, 58, 2, 13, 197, 12, 16, 43, 23, 2, 2, 62, 30, 145, 402, 11, 2, 51, 575, 32, 61, 369, 71, 66, 770, 12, 2, 75, 100, 2, 2, 2, 105, 37, 69, 147, 712, 75, 2, 44, 257, 390, 2, 69, 263, 514, 105, 50, 286, 2, 23, 2, 123, 13, 161, 40, 2, 421, 2, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 2, 121, 25, 70, 2, 2, 719, 2, 13, 18, 31, 62, 40, 2, 2, 2, 2, 2, 14, 123, 2, 942, 25, 2, 721, 12, 145, 2, 202, 12, 160, 580, 202, 12, 2, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 2, 15, 251, 2, 2, 12, 38, 84, 80, 124, 12, 2, 23]),\n",
       "         list([2, 17, 2, 194, 337, 2, 2, 204, 22, 45, 254, 2, 106, 14, 123, 2, 2, 270, 2, 2, 2, 2, 732, 2, 101, 405, 39, 14, 2, 2, 2, 2, 115, 50, 305, 12, 47, 2, 168, 2, 235, 2, 38, 111, 699, 102, 2, 2, 2, 2, 2, 24, 2, 78, 2, 17, 2, 2, 21, 27, 2, 2, 2, 2, 2, 92, 2, 2, 2, 2, 2, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 2, 97, 12, 157, 21, 2, 2, 2, 2, 66, 78, 2, 2, 631, 2, 2, 2, 272, 191, 2, 2, 2, 2, 2, 2, 2, 544, 2, 383, 2, 848, 2, 2, 497, 2, 2, 2, 2, 2, 21, 60, 27, 239, 2, 43, 2, 209, 405, 10, 10, 12, 764, 40, 2, 248, 20, 12, 16, 2, 174, 2, 72, 2, 51, 2, 2, 22, 2, 204, 131, 2])],\n",
       "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([list([2, 591, 202, 14, 31, 2, 717, 10, 10, 2, 2, 2, 2, 360, 2, 2, 177, 2, 394, 354, 2, 123, 2, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 2, 124, 14, 286, 170, 2, 157, 46, 2, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 2, 717]),\n",
       "         list([2, 14, 22, 2, 2, 176, 2, 2, 88, 12, 2, 23, 2, 2, 109, 943, 2, 114, 2, 55, 606, 2, 111, 2, 2, 139, 193, 273, 23, 2, 172, 270, 11, 2, 2, 2, 2, 2, 109, 2, 21, 2, 22, 2, 2, 2, 2, 2, 10, 10, 2, 105, 987, 35, 841, 2, 19, 861, 2, 2, 2, 2, 45, 55, 221, 15, 670, 2, 526, 14, 2, 2, 405, 2, 2, 2, 27, 85, 108, 131, 2, 2, 2, 2, 405, 2, 2, 133, 2, 50, 13, 104, 51, 66, 166, 14, 22, 157, 2, 2, 530, 239, 34, 2, 2, 45, 407, 31, 2, 41, 2, 105, 21, 59, 299, 12, 38, 950, 2, 2, 15, 45, 629, 488, 2, 127, 2, 52, 292, 17, 2, 2, 185, 132, 2, 2, 2, 488, 2, 47, 2, 392, 173, 2, 2, 2, 270, 2, 2, 2, 2, 2, 65, 55, 73, 11, 346, 14, 20, 2, 2, 976, 2, 2, 2, 861, 2, 2, 2, 30, 2, 2, 56, 2, 841, 2, 990, 692, 2, 2, 2, 398, 229, 10, 10, 13, 2, 670, 2, 14, 2, 31, 2, 27, 111, 108, 15, 2, 19, 2, 2, 875, 551, 14, 22, 2, 2, 21, 45, 2, 2, 45, 252, 2, 2, 2, 565, 921, 2, 39, 2, 529, 48, 25, 181, 2, 67, 35, 2, 22, 49, 238, 60, 135, 2, 14, 2, 290, 2, 58, 10, 10, 472, 45, 55, 878, 2, 169, 11, 374, 2, 25, 203, 28, 2, 818, 12, 125, 2, 2]),\n",
       "         list([2, 111, 748, 2, 2, 2, 2, 2, 87, 2, 2, 2, 31, 318, 2, 2, 2, 498, 2, 748, 63, 29, 2, 220, 686, 2, 2, 17, 12, 575, 220, 2, 17, 2, 185, 132, 2, 16, 53, 928, 11, 2, 74, 2, 438, 21, 27, 2, 589, 2, 22, 107, 2, 2, 997, 2, 2, 35, 2, 2, 11, 22, 231, 54, 29, 2, 29, 100, 2, 2, 34, 2, 2, 2, 2, 2, 98, 31, 2, 33, 2, 58, 14, 2, 2, 2, 2, 365, 2, 2, 2, 356, 346, 2, 2, 2, 63, 29, 93, 11, 2, 11, 2, 33, 2, 58, 54, 2, 431, 748, 2, 32, 2, 16, 11, 94, 2, 10, 10, 2, 993, 2, 2, 2, 2, 2, 2, 2, 2, 847, 2, 2, 121, 31, 2, 27, 86, 2, 2, 16, 2, 465, 993, 2, 2, 573, 17, 2, 42, 2, 2, 37, 473, 2, 711, 2, 2, 2, 328, 212, 70, 30, 258, 11, 220, 32, 2, 108, 21, 133, 12, 2, 55, 465, 849, 2, 53, 33, 2, 2, 37, 70, 2, 2, 2, 2, 74, 476, 37, 62, 91, 2, 169, 2, 2, 2, 146, 655, 2, 2, 258, 12, 184, 2, 546, 2, 849, 2, 2, 2, 22, 2, 18, 631, 2, 797, 2, 2, 2, 71, 348, 425, 2, 2, 19, 2, 2, 2, 11, 661, 2, 339, 2, 2, 2, 2, 2, 2, 2, 10, 10, 263, 787, 2, 270, 11, 2, 2, 2, 2, 2, 121, 2, 2, 26, 2, 19, 68, 2, 2, 28, 446, 2, 318, 2, 2, 67, 51, 36, 70, 81, 2, 2, 2, 36, 2, 2, 2, 2, 18, 2, 711, 2, 2, 26, 2, 2, 11, 14, 636, 720, 12, 426, 28, 77, 776, 2, 97, 38, 111, 2, 2, 168, 2, 2, 137, 2, 18, 27, 173, 2, 2, 17, 2, 2, 428, 2, 232, 11, 2, 2, 37, 272, 40, 2, 247, 30, 656, 2, 2, 54, 2, 2, 98, 2, 2, 40, 558, 37, 2, 98, 2, 2, 2, 15, 14, 2, 57, 2, 2, 2, 2, 275, 711, 2, 2, 2, 98, 2, 2, 10, 10, 2, 19, 14, 2, 267, 162, 711, 37, 2, 752, 98, 2, 2, 2, 90, 19, 2, 2, 2, 2, 2, 2, 2, 2, 2, 930, 2, 508, 90, 2, 2, 2, 2, 2, 17, 2, 2, 2, 2, 2, 2, 2, 189, 2, 2, 2, 2, 2, 2, 2, 95, 271, 23, 2, 2, 2, 2, 2, 33, 2, 2, 425, 2, 2, 2, 2, 2, 2, 2, 2, 469, 2, 2, 54, 2, 150, 2, 2, 280, 53, 2, 2, 18, 339, 29, 2, 27, 2, 2, 2, 68, 2, 19, 2, 2, 2, 2, 2, 263, 65, 2, 34, 2, 2, 2, 43, 159, 29, 2, 2, 2, 387, 73, 195, 584, 10, 10, 2, 2, 58, 810, 54, 14, 2, 117, 22, 16, 93, 2, 2, 2, 192, 15, 12, 16, 93, 34, 2, 2, 2, 33, 2, 2, 2, 15, 2, 2, 2, 325, 12, 62, 30, 776, 2, 67, 14, 17, 2, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 2, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 2, 2, 819, 2, 22, 2, 17, 2, 2, 787, 2, 2, 2, 2, 100, 30, 2, 2, 2, 2, 2, 42, 2, 11, 2, 2, 42, 101, 704, 2, 101, 999, 15, 2, 94, 2, 180, 2, 2, 2, 34, 2, 45, 2, 2, 22, 60, 2, 2, 31, 11, 94, 2, 96, 21, 94, 749, 2, 57, 975]),\n",
       "         ...,\n",
       "         list([2, 13, 2, 15, 2, 135, 14, 2, 35, 32, 46, 394, 20, 62, 30, 2, 21, 45, 184, 78, 2, 2, 910, 769, 2, 2, 395, 2, 2, 2, 11, 119, 2, 89, 2, 2, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 2, 185, 2, 284, 2, 2, 37, 315, 2, 226, 20, 272, 2, 40, 29, 152, 60, 181, 2, 30, 50, 553, 362, 80, 119, 12, 21, 846, 2]),\n",
       "         list([2, 11, 119, 241, 2, 2, 840, 20, 12, 468, 15, 94, 2, 562, 791, 39, 2, 86, 107, 2, 97, 14, 31, 33, 2, 2, 2, 743, 46, 2, 2, 2, 2, 2, 768, 47, 2, 79, 90, 145, 164, 162, 50, 2, 501, 119, 2, 2, 2, 78, 232, 15, 16, 224, 11, 2, 333, 20, 2, 985, 200, 2, 2, 2, 2, 2, 2, 79, 357, 2, 20, 47, 220, 57, 206, 139, 11, 12, 2, 55, 117, 212, 13, 2, 92, 124, 51, 45, 2, 71, 536, 13, 520, 14, 20, 2, 2, 2, 470]),\n",
       "         list([2, 2, 52, 2, 430, 22, 2, 220, 2, 2, 28, 2, 519, 2, 2, 769, 15, 47, 2, 2, 2, 2, 114, 2, 33, 222, 31, 55, 184, 704, 2, 2, 19, 346, 2, 2, 2, 364, 350, 2, 184, 2, 2, 133, 2, 11, 2, 2, 21, 2, 2, 2, 570, 50, 2, 2, 2, 2, 2, 17, 2, 2, 2, 21, 17, 2, 2, 232, 2, 2, 29, 266, 56, 96, 346, 194, 308, 2, 194, 21, 29, 218, 2, 19, 2, 78, 173, 2, 27, 2, 2, 2, 718, 2, 2, 2, 2, 17, 210, 2, 2, 2, 47, 77, 395, 14, 172, 173, 18, 2, 2, 2, 82, 127, 27, 173, 11, 2, 392, 217, 21, 50, 2, 57, 65, 12, 2, 53, 40, 35, 390, 2, 11, 2, 2, 2, 2, 314, 74, 2, 792, 22, 2, 19, 714, 727, 2, 382, 2, 91, 2, 439, 19, 14, 20, 2, 2, 2, 2, 2, 756, 25, 124, 2, 31, 12, 16, 93, 804, 34, 2, 2])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0])))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore the top 10 most frequent words using skip_top\n",
    "\n",
    "imdb.load_data(skip_top=10, num_words=1000, oov_char=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 518, 21, 55, 1713, 6, 20, 716, 6, 65, 38, 73, 15, 12, 220, 461, 878, 14, 20, 716, 450, 537, 38, 73, 5189, 15, 12, 16, 4, 86, 171, 211, 6, 20, 13, 100, 24, 106, 8, 20252, 12, 16, 99, 147, 5, 4, 105, 38, 565, 15, 149, 12, 877, 6, 965, 1651, 319, 134, 289, 349, 5, 68, 2166, 855, 19, 68, 10082, 31, 11, 843, 400, 569, 72, 99, 254, 150, 13, 28, 296, 11, 94, 6274, 209, 21501, 450, 211, 5, 13, 923, 51, 13, 210, 6677, 14, 20, 9, 6, 991, 4, 487, 4, 116, 4, 10409, 7, 450, 537, 209, 112, 60, 4, 222, 227, 5303, 285, 44, 14, 20, 9, 3160, 1542, 1809, 2128, 57, 594, 12, 434, 215, 28, 1816, 98]),\n",
       "         list([1, 14, 22, 714, 8012, 4, 921, 2124, 4, 905, 1488, 5, 4, 350, 2501, 354, 7, 1691, 1612, 349, 13, 1610, 12, 23, 6, 13574, 5, 16, 2664, 15, 13, 69, 24, 557, 7, 12, 159, 10, 10, 13, 81, 24, 124, 48, 14, 16, 14513, 3667, 2016, 21, 4, 1794, 4, 8466, 5, 943, 7, 4, 105, 17, 73, 17, 4, 49, 1096, 370, 157, 3392, 4, 109, 33241, 299, 32, 1467, 6, 1249, 744, 10, 10, 4, 8466, 200, 1593, 5, 14513, 1367, 4, 172, 389, 1175, 75, 219, 11, 1513, 890, 19, 1593, 5, 1441, 6909, 6239, 9, 389, 11, 41, 105, 1302, 4182, 5, 13291, 6, 8022, 23, 41, 105, 11, 33, 297, 11, 4, 5322, 7, 4, 1635, 59, 9, 2227, 5, 246, 31, 70, 9530, 19, 41, 33, 4, 172, 58, 10, 10, 50, 26, 49, 388, 121, 13, 235, 4, 114, 9281, 6, 1229, 5, 4, 388, 200, 33241, 5, 27, 1233, 980, 220, 306, 398, 18, 160, 22, 33241, 266, 125, 17, 160, 109, 32, 295, 21, 148, 26, 1403, 5266, 10, 10, 14, 22, 215, 30, 448, 23, 6, 283, 65, 42, 215, 28, 77, 398, 34, 294, 37, 1452, 134, 2490, 13, 967, 12, 709, 46, 7, 6, 878, 158, 10, 10]),\n",
       "         list([1, 13, 219, 14, 20, 23, 248, 5, 447, 12, 13, 244, 6, 147, 1690, 22, 337, 5, 14, 31, 16, 87, 4, 177, 16, 93, 7, 49, 66, 221, 84, 8246, 9, 210, 87, 5, 1024, 31711, 9, 11, 6, 2756, 7, 27, 205, 29, 70, 297, 199, 212, 5, 708, 11, 4, 172, 20, 40, 171, 409, 70, 4, 65, 347, 9, 87, 99, 4, 197, 7, 112, 502, 8, 794, 6, 58, 347, 7, 51, 80, 593, 5, 8, 361, 14, 58, 347, 8, 3621, 6, 4564, 1690, 9, 35, 221, 326, 5, 14, 20, 961, 12, 46, 11, 141, 6, 96, 15, 9, 220, 484, 867])],\n",
       "        dtype=object), array([1, 0, 0, ..., 1, 1, 1])),\n",
       " (array([list([1, 14, 9, 31, 7, 4, 249, 108, 13, 28, 110, 11, 6, 137, 10, 10, 4, 439, 9, 15, 12, 152, 124, 726, 12, 494, 8, 30, 35, 1089, 993, 22, 43675, 42, 35, 3435, 9, 10528, 17, 6, 959, 12, 996, 23, 32, 6566, 10, 10, 4, 116, 9, 2526, 4, 2559, 125, 1489, 5, 4, 424, 3881, 1149, 10, 10, 14746, 8035, 9, 242, 4, 118, 155, 44, 14, 22, 21, 15, 218, 6, 52, 155, 252, 29, 47, 35, 1596, 5, 61226, 168, 21, 1116, 29, 191, 165, 511, 43, 168, 33, 89, 29, 12482, 54, 27, 4727, 889, 10, 10, 66, 92, 106, 14, 22, 49, 135, 12, 738, 3260, 4719, 13, 135, 31, 9, 99, 111]),\n",
       "         list([1, 4, 7591, 248, 7, 108, 5077, 28, 13, 421, 38, 117, 11728, 8, 105, 5077, 28, 13, 77, 93, 8, 4032, 34, 141, 3648, 414, 15670, 1316, 3264, 7678, 28837, 2161, 8464, 2986, 732, 12435, 61019, 46432, 798, 14, 22, 17, 48, 12, 71, 129, 24558]),\n",
       "         list([1, 160, 12576, 212, 270, 11, 4, 1547, 12, 186, 15, 4, 612, 31, 1622, 1466, 18, 1205, 16, 11, 14, 172, 719, 1547, 17, 38, 111, 7, 12563, 5, 71375, 108, 26, 270, 50, 5, 137, 14, 9, 246, 160, 31, 12, 9, 275, 195, 5, 73, 93, 15, 13, 131, 510, 12, 10, 10, 16866, 9, 928, 11, 6, 1155, 74, 644, 267, 5, 116, 7376, 30258, 13, 104, 442, 424, 8, 30, 6, 117, 1155, 151, 11, 41, 3992, 523, 59, 9, 99, 185, 8, 30, 928, 11, 349, 73, 16866, 127, 24, 1497, 41, 1417, 5, 515, 29, 5, 7376, 521, 245, 18, 49, 1356, 253, 183, 79, 2732, 54, 4, 3992, 106, 9, 2586, 16866, 659, 12, 5, 408, 12, 8, 7376, 17, 6, 3470, 5, 111, 712, 959, 10, 10, 542, 1794, 5, 4, 192, 15, 14, 20, 122, 24, 5390, 99, 76, 23, 706, 2764, 21, 6, 3793, 114, 97, 14, 6, 1036, 5, 737, 117, 22]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 0, 1, ..., 0, 0, 0])))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Limit the sequence lengths to 500 using maxlen\n",
    "\n",
    "imdb.load_data(maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
       "         ...,\n",
       "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
       "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
       "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
       "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
       "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
       "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
       "         ...,\n",
       "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
       "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
       "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
       "        dtype=object),\n",
       "  array([0, 1, 1, ..., 0, 0, 0])))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use '1' as the character that indicates the start of a sequence\n",
    "\n",
    "imdb.load_data(start_char=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the imdb word index using get_word_index()\n",
    "\n",
    "imdb_word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the word index as a dictionary,\n",
    "# accounting for index_from.\n",
    "\n",
    "index_from =3\n",
    "imdb_word_index = {key: value + index_from for key , value in imdb_word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52256"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve a specific word's index\n",
    "imdb_word_index['simpsonian']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an input sentence\n",
    "\n",
    "imdb_word_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Padding and Masking Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb data set\n",
    "import tensorflow.keras.datasets.imdb as imdb\n",
    "\n",
    "(x_train, y_train),(x_test, y_test)= imdb.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the input data shape\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the inputs to the maximum length using maxlen\n",
    "padded_x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=300, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the output data shape\n",
    "padded_x_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Masking layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking expects to see (batch, sequence, features)\n",
    "# Create a dummy feature dimension using expand_dims\n",
    "\n",
    "padded_x_train = np.expand_dims(padded_x_train,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Masking layer \n",
    "\n",
    "tf_x_train = tf.convert_to_tensor(padded_x_train, dtype='float32')\n",
    "masking_layer = tf.keras.layers.Masking(mask_value = 0.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass tf_x_train to it\n",
    "masked_x_train = masking_layer(tf_x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7, shape=(25000, 300, 1), dtype=float32, numpy=\n",
       "array([[[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [2.200e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.940e+02],\n",
       "        [1.153e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.400e+01],\n",
       "        [4.700e+01],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.100e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.446e+03],\n",
       "        [7.079e+03],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]],\n",
       "\n",
       "       [[1.000e+00],\n",
       "        [1.700e+01],\n",
       "        [6.000e+00],\n",
       "        ...,\n",
       "        [0.000e+00],\n",
       "        [0.000e+00],\n",
       "        [0.000e+00]]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the dataset\n",
    "\n",
    "masked_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8, shape=(25000, 300), dtype=bool, numpy=\n",
       "array([[ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False],\n",
       "       [ True,  True,  True, ..., False, False, False]])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the ._keras_mask for the dataset\n",
    "masked_x_train._keras_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## The Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer using layers.Embedding\n",
    "# Specify input_dim, output_dim, input_length\n",
    "\n",
    "embedding_layer  = tf.keras.layers.Embedding(input_dim=501, output_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(1, 4, 1, 16), dtype=float32, numpy=\n",
       "array([[[[-0.04580507,  0.02271153, -0.01393714, -0.01548839,\n",
       "          -0.02871852,  0.03213588,  0.00092607,  0.02071251,\n",
       "           0.04931721,  0.02751285, -0.00049518,  0.02391819,\n",
       "           0.04213348, -0.01607733, -0.01797171,  0.00972332]],\n",
       "\n",
       "        [[-0.03392782, -0.02044195, -0.03529134,  0.03888642,\n",
       "          -0.03557805,  0.04969717,  0.00769024, -0.04493414,\n",
       "           0.03907165,  0.02650625,  0.02962983,  0.02120482,\n",
       "          -0.0195017 , -0.00108335,  0.02921681, -0.01730379]],\n",
       "\n",
       "        [[-0.0400693 , -0.04159235,  0.03584364,  0.04514314,\n",
       "           0.00049062, -0.0027938 ,  0.02626849, -0.02475668,\n",
       "          -0.02771629,  0.03298155,  0.0036453 , -0.02566025,\n",
       "           0.04691326, -0.02741792,  0.00848998, -0.03276463]],\n",
       "\n",
       "        [[ 0.02464792, -0.02704152,  0.0270244 ,  0.04619328,\n",
       "           0.01648794, -0.04824672,  0.00375921, -0.03626733,\n",
       "           0.04705996,  0.02074729, -0.03095313, -0.02119545,\n",
       "          -0.01713436, -0.01860813, -0.02055589,  0.02932784]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect an Embedding layer output for a fixed input\n",
    "# Expects an input of shape (batch, sequence, feature)\n",
    "sequence_of_indices = tf.constant([[[0],[1],[5],[500]]])\n",
    "sequence_of_embaeddings = embedding_layer(sequence_of_indices)\n",
    "sequence_of_embaeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04580507,  0.02271153, -0.01393714, ..., -0.01607733,\n",
       "        -0.01797171,  0.00972332],\n",
       "       [-0.03392782, -0.02044195, -0.03529134, ..., -0.00108335,\n",
       "         0.02921681, -0.01730379],\n",
       "       [ 0.02850748, -0.047007  ,  0.00531486, ...,  0.03904343,\n",
       "         0.01036897,  0.02056103],\n",
       "       ...,\n",
       "       [-0.03886176, -0.02142171,  0.03747387, ...,  0.02767618,\n",
       "         0.03558494, -0.01874942],\n",
       "       [-0.00529613, -0.03980184,  0.043152  , ...,  0.04694817,\n",
       "        -0.02535688, -0.01886815],\n",
       "       [ 0.02464792, -0.02704152,  0.0270244 , ..., -0.01860813,\n",
       "        -0.02055589,  0.02932784]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the Embedding layer weights using get_weights()\n",
    "\n",
    "embedding_layer.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02838257, -0.03163974, -0.00752687,  0.02433052, -0.04002273,\n",
       "       -0.01682455,  0.03920564,  0.01282619, -0.04472236,  0.04278984,\n",
       "        0.04703097, -0.03228743, -0.01546295,  0.02071146,  0.02182115,\n",
       "       -0.03619404], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the 14th index\n",
    "\n",
    "embedding_layer.get_weights()[0][14, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a layer that uses the mask_zero kwarg\n",
    "masking_embadding_layer = tf.keras.layers.Embedding(input_dim=501, output_dim=16, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=44, shape=(1, 4, 1), dtype=bool, numpy=\n",
       "array([[[False],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True]]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply this layer to the sequence and see the _keras_mask property\n",
    "masked_sequence_of_embeddings = masking_embadding_layer(sequence_of_indices)\n",
    "masked_sequence_of_embeddings._keras_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## The Embedding Projector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and preprocess the IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(x_train,y_train),(x_test, y_test) = get_and_pad_imdb_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index\n",
    "\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the keys and values of the word index\n",
    "inv_imdb_word_index = {value: key for key, value in imdb_word_index.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'a',\n",
       " 'great',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'and',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'that',\n",
       " \"he's\",\n",
       " 'made',\n",
       " 'on',\n",
       " 'dvd',\n",
       " 'except',\n",
       " 'for',\n",
       " 'hotel',\n",
       " 'room',\n",
       " 'the',\n",
       " '2',\n",
       " 'hour',\n",
       " 'twin',\n",
       " 'peaks',\n",
       " 'movie',\n",
       " 'so',\n",
       " 'when',\n",
       " 'i',\n",
       " 'found',\n",
       " 'out',\n",
       " 'about',\n",
       " 'this',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'grabbed',\n",
       " 'it',\n",
       " 'and',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'this',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'drawn',\n",
       " 'black',\n",
       " 'and',\n",
       " 'white',\n",
       " 'cartoons',\n",
       " 'that',\n",
       " 'are',\n",
       " 'loud',\n",
       " 'and',\n",
       " 'foul',\n",
       " 'mouthed',\n",
       " 'and',\n",
       " 'unfunny',\n",
       " 'maybe',\n",
       " 'i',\n",
       " \"don't\",\n",
       " 'know',\n",
       " \"what's\",\n",
       " 'good',\n",
       " 'but',\n",
       " 'maybe',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'crap',\n",
       " 'that',\n",
       " 'was',\n",
       " 'on',\n",
       " 'the',\n",
       " 'public',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'david',\n",
       " 'lynch',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'few',\n",
       " 'bucks',\n",
       " 'too',\n",
       " 'let',\n",
       " 'me',\n",
       " 'make',\n",
       " 'it',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'i',\n",
       " \"didn't\",\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'foul',\n",
       " 'language',\n",
       " 'part',\n",
       " 'but',\n",
       " 'had',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'because',\n",
       " 'my',\n",
       " 'neighbors',\n",
       " 'might',\n",
       " 'have',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'highly',\n",
       " 'disappointing',\n",
       " 'release',\n",
       " 'and',\n",
       " 'may',\n",
       " 'well',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'left',\n",
       " 'in',\n",
       " 'the',\n",
       " 'box',\n",
       " 'set',\n",
       " 'as',\n",
       " 'a',\n",
       " 'curiosity',\n",
       " 'i',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'you',\n",
       " \"don't\",\n",
       " 'spend',\n",
       " 'your',\n",
       " 'money',\n",
       " 'on',\n",
       " 'this',\n",
       " '2',\n",
       " 'out',\n",
       " 'of',\n",
       " '10']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first dataset example sentence\n",
    "\n",
    "[inv_imdb_word_index[index] for index in x_train[100] if index >2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build an Embedding layer into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum token value\n",
    "max_index_value = max(imdb_word_index.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an embedding dimension\n",
    "embedding_dim = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model using Sequential:\n",
    "#     1. Embedding layer\n",
    "#     2. GlobalAveragePooling1D\n",
    "#     3. Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=embedding_dim, mask_zero=False),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(units=1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API refresher: use the Model to build the same model\n",
    "\n",
    "# Functional API\n",
    "# Functional API\n",
    "# Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile, train, and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a binary cross-entropy loss\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 33s 1ms/sample - loss: 0.6902 - accuracy: 0.5701 - val_loss: 0.0175 - val_accuracy: 0.5562\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 32s 1ms/sample - loss: 0.6714 - accuracy: 0.6646 - val_loss: 0.0167 - val_accuracy: 0.6844\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 32s 1ms/sample - loss: 0.6289 - accuracy: 0.7573 - val_loss: 0.0154 - val_accuracy: 0.7609\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.5765 - accuracy: 0.7902 - val_loss: 0.0142 - val_accuracy: 0.7781\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 31s 1ms/sample - loss: 0.5267 - accuracy: 0.8181 - val_loss: 0.0130 - val_accuracy: 0.8047\n"
     ]
    }
   ],
   "source": [
    "# Train the model using .fit(), savng its history\n",
    "history=model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test,y_test), validation_steps=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The TensorFlow embedding projector\n",
    "\n",
    "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding layer's weights from the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the word Embeddings to tsv files\n",
    "# Two files: \n",
    "#     one contains the embedding labels (meta.tsv),\n",
    "#     one contains the embeddings (vecs.tsv)\n",
    "\n",
    "import io\n",
    "from os import path\n",
    "\n",
    "out_v = io.open(path.join('data', 'vecs.tsv'), 'w', encoding='utf-8')\n",
    "out_m = io.open(path.join('data', 'meta.tsv'), 'w', encoding='utf-8')\n",
    "\n",
    "k = 0\n",
    "\n",
    "for word, token in word_index.items():\n",
    "    if k != 0:\n",
    "        out_m.write('\\n')\n",
    "        out_v.write('\\n')\n",
    "    \n",
    "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
    "    out_m.write(word)\n",
    "    k += 1\n",
    "    \n",
    "out_v.close()\n",
    "out_m.close()\n",
    "# beware large collections of embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Recurrent neural network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize and pass an input to a SimpleRNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleRNN layer and test it\n",
    "simplernn_layer = tf.keras.layers.SimpleRNN(units=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=195, shape=(1, 16), dtype=float32, numpy=\n",
       "array([[-0.9999998, -1.       , -1.       ,  1.       ,  1.       ,\n",
       "        -1.       ,  0.9999998,  1.       ,  1.       , -1.       ,\n",
       "        -1.       , -1.       ,  1.       ,  1.       , -0.9999998,\n",
       "         1.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that only the final cell output is returned\n",
    "\n",
    "sequence = tf.constant([[[1.,1.],[2.,2.],[56.,-100.]]])\n",
    "layer_output = simplernn_layer(sequence)\n",
    "layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train),(x_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "\n",
    "imdb_word_index = get_imdb_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a recurrent neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value\n",
    "\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build the model:\n",
    "# 1. Embedding.\n",
    "# 2. LSTM.\n",
    "# 3. Dense.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_index_value, output_dim=embedding_dim, mask_zero = True),\n",
    "    tf.keras.layers.LSTM(units=16),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with binary cross-entropy loss\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 640s 26ms/sample - loss: 0.3995 - accuracy: 0.8184\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 590s 24ms/sample - loss: 0.2261 - accuracy: 0.9164\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 585s 23ms/sample - loss: 0.1652 - accuracy: 0.9406\n"
     ]
    }
   ],
   "source": [
    "# Fit the model and save its training history\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size= 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "#val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "#val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "#plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ignore',\n",
       " 'the',\n",
       " 'bad',\n",
       " 'reviews',\n",
       " 'on',\n",
       " 'here',\n",
       " 'this',\n",
       " 'film',\n",
       " 'is',\n",
       " 'awesome',\n",
       " 'just',\n",
       " 'before',\n",
       " 'dawn',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'example',\n",
       " 'of',\n",
       " 'what',\n",
       " 'can',\n",
       " 'be',\n",
       " 'done',\n",
       " 'in',\n",
       " 'a',\n",
       " 'film',\n",
       " 'with',\n",
       " 'a',\n",
       " 'minimal',\n",
       " 'budget',\n",
       " 'if',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'dedicated',\n",
       " 'crew',\n",
       " 'decent',\n",
       " 'script',\n",
       " 'and',\n",
       " 'a',\n",
       " 'cool',\n",
       " 'idea',\n",
       " 'for',\n",
       " 'a',\n",
       " 'film',\n",
       " \"it's\",\n",
       " 'a',\n",
       " 'hell',\n",
       " 'of',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'fun',\n",
       " 'br',\n",
       " 'br',\n",
       " 'i',\n",
       " 'enjoyed',\n",
       " 'it',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'more',\n",
       " 'than',\n",
       " 'most',\n",
       " 'other',\n",
       " \"80's\",\n",
       " 'slashers',\n",
       " 'because',\n",
       " 'the',\n",
       " 'killer',\n",
       " 'is',\n",
       " 'so',\n",
       " 'unique',\n",
       " 'wrong',\n",
       " 'turn',\n",
       " 'ripped',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'off',\n",
       " 'something',\n",
       " 'fierce',\n",
       " \"there's\",\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'blood',\n",
       " 'and',\n",
       " 'scares',\n",
       " 'my',\n",
       " 'girlfriend',\n",
       " 'was',\n",
       " 'freaked',\n",
       " 'out',\n",
       " 'and',\n",
       " 'she',\n",
       " 'watches',\n",
       " 'almost',\n",
       " 'everything',\n",
       " 'with',\n",
       " 'me',\n",
       " 'and',\n",
       " \"doesn't\",\n",
       " \"it's\",\n",
       " 'got',\n",
       " 'that',\n",
       " 'creepiness',\n",
       " 'to',\n",
       " 'it',\n",
       " 'br',\n",
       " 'br',\n",
       " \"i'd\",\n",
       " 'say',\n",
       " 'that',\n",
       " 'just',\n",
       " 'before',\n",
       " 'dawn',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'early',\n",
       " \"80's\",\n",
       " 'slasher',\n",
       " 'out',\n",
       " 'there',\n",
       " 'i',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'it',\n",
       " 'br',\n",
       " 'br',\n",
       " '8',\n",
       " 'out',\n",
       " 'of',\n",
       " '10',\n",
       " 'kids']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first test data example sentence\n",
    "# (invert the word index)\n",
    "inv_imdb_word_index= {value: key for key, value in imdb_word_index.items()}\n",
    "[inv_imdb_word_index[index] for index in x_test[0] if index > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99494886]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the model prediction using model.predict()\n",
    "model.predict(x_test[None, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the corresponding label\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"coding_tutorial_6\"></a>\n",
    "## Stacked RNNs and the Bidirectional wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and transform the IMDB review sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load and preprocess the IMDB dataset\n",
    "\n",
    "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "\n",
    "    # Load the reviews\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
    "                                                          num_words=num_words,\n",
    "                                                          skip_top=0,\n",
    "                                                          maxlen=maxlen,\n",
    "                                                          start_char=1,\n",
    "                                                          oov_char=2,\n",
    "                                                          index_from=index_from)\n",
    "\n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
    "                                                        maxlen=None,\n",
    "                                                        padding='pre',\n",
    "                                                        truncating='pre',\n",
    "                                                        value=0)\n",
    "    \n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                           maxlen=None,\n",
    "                                                           padding='pre',\n",
    "                                                           truncating='pre',\n",
    "                                                           value=0)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_and_pad_imdb_dataset(maxlen=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to get the dataset word index\n",
    "\n",
    "def get_imdb_word_index(num_words=10000, index_from=2):\n",
    "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
    "                                        path='imdb_word_index.json')\n",
    "    imdb_word_index = {key: value + index_from for\n",
    "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
    "    return imdb_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the word index using get_imdb_word_index()\n",
    "imdb_word_index = get_imdb_word_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build stacked and bidirectional recurrent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum index value and specify an embedding dimension\n",
    "\n",
    "max_index_value = max(imdb_word_index.values())\n",
    "embedding_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_index_value, output_dim=embedding_dim, mask_zero = True),\n",
    "    tf.keras.layers.LSTM(units=32, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=32, return_sequences=False),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_index_value, output_dim=embedding_dim, mask_zero = True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=8), merge_mode='sum'),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=max_index_value, output_dim=embedding_dim, mask_zero = True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=8, return_sequences=True),merge_mode='concat'),\n",
    "    tf.keras.layers.GRU(units=8, return_sequences=False),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, saving its history\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=3, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc      = history_dict['accuracy']\n",
    "val_acc  = history_dict['val_accuracy']\n",
    "loss     = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
    "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim(0, 1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
